[
  {
    "objectID": "notebooks/us_accidents_analysis.html",
    "href": "notebooks/us_accidents_analysis.html",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "",
    "text": "Data Retrieval"
  },
  {
    "objectID": "notebooks/us_accidents_analysis.html#data-source",
    "href": "notebooks/us_accidents_analysis.html#data-source",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "1.1 Data Source",
    "text": "1.1 Data Source\nThe dataset used in this project is the US Accidents (3.0 Million records) dataset sourced from Kaggle.\n\nSource: Kaggle - US Accidents Dataset\nFormat: CSV\nContents: Accident data including location, time, weather conditions, and environment features.\n\n\n\nCode\n# Import required libraries\nimport pandas as pd\n\n# Load the accidents data\naccidents_df = pd.read_csv('../data/US_Accidents_March23.csv')  # Adjust path if needed\n\n# Show basic information\nprint(f\"Shape of dataset: {accidents_df.shape}\")\naccidents_df.head()\n\n\nShape of dataset: (7728394, 46)\n\n\n\n\n\n\n\n\n\nID\nSource\nSeverity\nStart_Time\nEnd_Time\nStart_Lat\nStart_Lng\nEnd_Lat\nEnd_Lng\nDistance(mi)\n...\nRoundabout\nStation\nStop\nTraffic_Calming\nTraffic_Signal\nTurning_Loop\nSunrise_Sunset\nCivil_Twilight\nNautical_Twilight\nAstronomical_Twilight\n\n\n\n\n0\nA-1\nSource2\n3\n2016-02-08 05:46:00\n2016-02-08 11:00:00\n39.865147\n-84.058723\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nNight\nNight\nNight\n\n\n1\nA-2\nSource2\n2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\n39.928059\n-82.831184\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nNight\nNight\nDay\n\n\n2\nA-3\nSource2\n2\n2016-02-08 06:49:27\n2016-02-08 07:19:27\n39.063148\n-84.032608\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nNight\nNight\nDay\nDay\n\n\n3\nA-4\nSource2\n3\n2016-02-08 07:23:34\n2016-02-08 07:53:34\n39.747753\n-84.205582\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nDay\nDay\nDay\n\n\n4\nA-5\nSource2\n2\n2016-02-08 07:39:07\n2016-02-08 08:09:07\n39.627781\n-84.188354\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nDay\nDay\nDay\nDay\n\n\n\n\n5 rows √ó 46 columns\n\n\n\n2.1 Convert Time Columns to Datetime Format\n\n\nCode\n# 2.1 Convert Time Columns to Datetime Format\n# Convert 'Start_Time' and 'End_Time' to proper datetime objects\naccidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], errors='coerce')\naccidents_df['End_Time'] = pd.to_datetime(accidents_df['End_Time'], errors='coerce')\n\n# Confirm that conversion was successful\nprint(\"Start_Time type after conversion:\", accidents_df['Start_Time'].dtype)\nprint(\"End_Time type after conversion:\", accidents_df['End_Time'].dtype)\n\n\nStart_Time type after conversion: datetime64[ns]\nEnd_Time type after conversion: datetime64[ns]\n\n\nC:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_16868\\4007129742.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  accidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], errors='coerce')\nC:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_16868\\4007129742.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  accidents_df['End_Time'] = pd.to_datetime(accidents_df['End_Time'], errors='coerce')\n\n\n2.2 Create New Feature: Extract Hour from Start_Time\n\n\nCode\n# Extract hour of accident from Start_Time\naccidents_df['Hour'] = accidents_df['Start_Time'].dt.hour\n\n# View distribution of accidents by hour\naccidents_df['Hour'].value_counts().sort_index()\n\n\nC:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_16868\\4178158478.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  accidents_df['Hour'] = accidents_df['Start_Time'].dt.hour\n\n\nHour\n0.0      98452\n1.0      85743\n2.0      82394\n3.0      74229\n4.0     149077\n5.0     209579\n6.0     375179\n7.0     546789\n8.0     541643\n9.0     334067\n10.0    313625\n11.0    322215\n12.0    316904\n13.0    352361\n14.0    394697\n15.0    463389\n16.0    520177\n17.0    516626\n18.0    390621\n19.0    267045\n20.0    201883\n21.0    169500\n22.0    148605\n23.0    110428\nName: count, dtype: int64\n\n\n2.3 Handle Missing Values\n\n\nCode\n# Check missing values summary\nmissing_values = accidents_df.isnull().sum()\nmissing_percentage = (missing_values / len(accidents_df)) * 100\n\nmissing_summary = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage (%)': missing_percentage\n})\nmissing_summary = missing_summary[missing_summary['Missing Values'] &gt; 0].sort_values(by='Percentage (%)', ascending=False)\n\nprint(\"Summary of Missing Values:\")\nmissing_summary\n\n\nSummary of Missing Values:\n\n\n\n\n\n\n\n\n\nMissing Values\nPercentage (%)\n\n\n\n\nStart_Time\n743166\n9.616047\n\n\nEnd_Time\n743166\n9.616047\n\n\nHour\n743166\n9.616047\n\n\nWeather_Condition\n173459\n2.244438\n\n\nTemperature(F)\n163853\n2.120143\n\n\nCity\n253\n0.003274\n\n\n\n\n\n\n\n2.4 Select Useful Columns for Analysis\n\n\nCode\n# 2.4 Select Useful Columns for Analysis\n\nuseful_columns = [\n    'ID', 'Start_Time', 'End_Time', 'State', 'City',\n    'Start_Lat', 'Start_Lng', 'Temperature(F)', 'Weather_Condition', 'Hour'\n]\n\naccidents_df = accidents_df[useful_columns]\n\n# Check the shape and preview of the cleaned dataset\nprint(f\"Shape after selecting useful columns: {accidents_df.shape}\")\naccidents_df.head()\n\n\nShape after selecting useful columns: (7728394, 10)\n\n\n\n\n\n\n\n\n\nID\nStart_Time\nEnd_Time\nState\nCity\nStart_Lat\nStart_Lng\nTemperature(F)\nWeather_Condition\nHour\n\n\n\n\n0\nA-1\n2016-02-08 05:46:00\n2016-02-08 11:00:00\nOH\nDayton\n39.865147\n-84.058723\n36.9\nLight Rain\n5.0\n\n\n1\nA-2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\nOH\nReynoldsburg\n39.928059\n-82.831184\n37.9\nLight Rain\n6.0\n\n\n2\nA-3\n2016-02-08 06:49:27\n2016-02-08 07:19:27\nOH\nWilliamsburg\n39.063148\n-84.032608\n36.0\nOvercast\n6.0\n\n\n3\nA-4\n2016-02-08 07:23:34\n2016-02-08 07:53:34\nOH\nDayton\n39.747753\n-84.205582\n35.1\nMostly Cloudy\n7.0\n\n\n4\nA-5\n2016-02-08 07:39:07\n2016-02-08 08:09:07\nOH\nDayton\n39.627781\n-84.188354\n36.0\nMostly Cloudy\n7.0"
  },
  {
    "objectID": "notebooks/regression_modeling.html",
    "href": "notebooks/regression_modeling.html",
    "title": "5. Regression Modelling",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the dataset\naccidents_df = pd.read_csv(\"/Users/anshureddy/Desktop/dwproject/accidents_cleaned.csv\")\n\n# Convert categorical variables to numeric using Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\naccidents_df['Weather_Condition'] = label_encoder.fit_transform(accidents_df['Weather_Condition'])\naccidents_df['State'] = label_encoder.fit_transform(accidents_df['State'])\naccidents_df['City'] = label_encoder.fit_transform(accidents_df['City'])\n\n# Group accidents by 'Hour' and calculate the frequency (count) of accidents per hour\naccidents_by_hour = accidents_df.groupby('Hour').size().reset_index(name='Accident_Frequency')\n\n# Merge the accident frequency with the original dataset on 'Hour'\naccidents_hourly = pd.merge(accidents_df, accidents_by_hour[['Hour', 'Accident_Frequency']], on='Hour')\n\n# Select relevant features for prediction\nX = accidents_hourly[['Weather_Condition', 'Temperature(F)', 'State', 'City']]\ny = accidents_hourly['Accident_Frequency']\n\n\n\n\n\n\n\nCode\n# Split the data into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the Linear Regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predict accident frequency on the test set\ny_pred_linear = linear_model.predict(X_test)\n\n# Evaluate the Linear Regression model\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\n\n# Print the results\nprint(\"Linear Regression Results - Predicting Accident Frequency:\")\nprint(f\"  Mean Squared Error (MSE): {mse_linear:.2f}\")\nprint(f\"  R-squared (R¬≤): {r2_linear:.2f}\")\n\n# Print the model coefficients\nprint(\"Linear Regression Coefficients - Influence of Each Feature on Accident Frequency:\")\nfor feature, coef in zip(X.columns, linear_model.coef_):\n    print(f\"  {feature}: {coef:.4f}\")\n\n# Include explanation for Weather_Condition coefficient\nprint(\"\\nExplanation of Features:\")\nprint(\"  - Weather_Condition is numerically encoded (e.g., 0 = Clear, 1 = Rainy, 2 = Snowy, etc.).\")\nprint(\"  - For each unit increase in Weather_Condition, the predicted accident frequency increases by 230.45 accidents.\")\nprint(\"  - Temperature(F) represents the effect of temperature on accidents, with each 1¬∞F increase leading to an increase of 777.99 accidents.\")\nprint(\"  - State is encoded numerically, with each unit increase in State leading to an increase of 439.48 accidents.\")\nprint(\"  - City has a negative relationship, with each unit increase in City corresponding to a decrease of 0.0574 accidents.\")\n\n\nLinear Regression Results - Predicting Accident Frequency:\n  Mean Squared Error (MSE): 17968374622.93\n  R-squared (R¬≤): 0.02\nLinear Regression Coefficients - Influence of Each Feature on Accident Frequency:\n  Weather_Condition: 230.4524\n  Temperature(F): 777.9901\n  State: 439.4800\n  City: -0.0574\n\nExplanation of Features:\n  - Weather_Condition is numerically encoded (e.g., 0 = Clear, 1 = Rainy, 2 = Snowy, etc.).\n  - For each unit increase in Weather_Condition, the predicted accident frequency increases by 230.45 accidents.\n  - Temperature(F) represents the effect of temperature on accidents, with each 1¬∞F increase leading to an increase of 777.99 accidents.\n  - State is encoded numerically, with each unit increase in State leading to an increase of 439.48 accidents.\n  - City has a negative relationship, with each unit increase in City corresponding to a decrease of 0.0574 accidents.\n\n\n\n\n\n\n\nCode\nfrom sklearn.linear_model import Ridge\n\n# Fit Ridge Regression model (with L2 regularization)\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_train, y_train)\n\n# Predict accident frequency on the test set\ny_pred_ridge = ridge_model.predict(X_test)\n\n# Evaluate the Ridge model\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nr2_ridge = r2_score(y_test, y_pred_ridge)\n\n# Print the results\nprint(\"Ridge Regression Results - Predicting Accident Frequency:\")\nprint(f\"  Mean Squared Error (MSE): {mse_ridge:.2f}\")\nprint(f\"  R-squared (R¬≤): {r2_ridge:.2f}\")\n\nprint(\"Ridge Regression Coefficients - Influence of Each Feature on Accident Frequency:\")\nfor feature, coef in zip(X.columns, ridge_model.coef_):\n    print(f\"  {feature}: {coef:.4f}\")\n\n# Include explanation for Weather_Condition coefficient in Ridge Regression\nprint(\"\\nExplanation of Features:\")\nprint(\"  - Weather_Condition is numerically encoded (e.g., 0 = Clear, 1 = Rainy, 2 = Snowy, etc.).\")\nprint(\"  - For each unit increase in Weather_Condition, the predicted accident frequency increases by 230.45 accidents.\")\nprint(\"  - Temperature(F) represents the effect of temperature on accidents, with each 1¬∞F increase leading to an increase of 777.99 accidents.\")\nprint(\"  - State is encoded numerically, with each unit increase in State leading to an increase of 439.48 accidents.\")\nprint(\"  - City has a negative relationship, with each unit increase in City corresponding to a decrease of 0.0574 accidents.\")\n\n\nRidge Regression Results - Predicting Accident Frequency:\n  Mean Squared Error (MSE): 17968374622.94\n  R-squared (R¬≤): 0.02\nRidge Regression Coefficients - Influence of Each Feature on Accident Frequency:\n  Weather_Condition: 230.4524\n  Temperature(F): 777.9901\n  State: 439.4800\n  City: -0.0574\n\nExplanation of Features:\n  - Weather_Condition is numerically encoded (e.g., 0 = Clear, 1 = Rainy, 2 = Snowy, etc.).\n  - For each unit increase in Weather_Condition, the predicted accident frequency increases by 230.45 accidents.\n  - Temperature(F) represents the effect of temperature on accidents, with each 1¬∞F increase leading to an increase of 777.99 accidents.\n  - State is encoded numerically, with each unit increase in State leading to an increase of 439.48 accidents.\n  - City has a negative relationship, with each unit increase in City corresponding to a decrease of 0.0574 accidents.\n\n\n\n\n\n\n\nCode\n# Evaluate performance on test data\nmse_linear_test = mean_squared_error(y_test, y_pred_linear)\nr2_linear_test = r2_score(y_test, y_pred_linear)\n\nmse_ridge_test = mean_squared_error(y_test, y_pred_ridge)\nr2_ridge_test = r2_score(y_test, y_pred_ridge)\n\n# Print test performance\nprint(\"Model Evaluation on Test Data - Comparison of Linear and Ridge Regression:\")\nprint(f\"Linear Regression -&gt; MSE: {mse_linear_test:.2f}, R¬≤: {r2_linear_test:.2f}\")\nprint(f\"Ridge Regression  -&gt; MSE: {mse_ridge_test:.2f}, R¬≤: {r2_ridge_test:.2f}\")\n\n\nModel Evaluation on Test Data - Comparison of Linear and Ridge Regression:\nLinear Regression -&gt; MSE: 17968374622.93, R¬≤: 0.02\nRidge Regression  -&gt; MSE: 17968374622.94, R¬≤: 0.02"
  },
  {
    "objectID": "notebooks/regression_modeling.html#summary-statistics",
    "href": "notebooks/regression_modeling.html#summary-statistics",
    "title": "5. Regression Modelling",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\nimport pandas as pd\n\n# Load the dataset\naccidents_df = pd.read_csv(\"/Users/anshureddy/Desktop/dwproject/accidents_cleaned.csv\")\n\n# Summary statistics for numerical columns (Temperature(F), Start_Lat, Start_Lng, Hour)\nnumerical_summary = accidents_df[['Temperature(F)', 'Start_Lat', 'Start_Lng', 'Hour']].describe()\nprint(\"Numerical Data Summary:\")\nprint(numerical_summary)\n\n# Summary statistics for categorical columns (State, City, Weather_Condition)\ncategorical_summary = accidents_df[['State', 'City', 'Weather_Condition']].describe()\nprint(\"\\nCategorical Data Summary:\")\nprint(categorical_summary)\n\n# Check for missing values\nmissing_values = accidents_df.isnull().sum()  # Count of missing values in each column\nprint(\"\\nMissing Values:\")\nprint(missing_values)\n\n# Correlation matrix for numerical columns\ncorrelation_matrix = accidents_df[['Temperature(F)', 'Start_Lat', 'Start_Lng', 'Hour']].corr()\nprint(\"\\nCorrelation Matrix:\")\nprint(correlation_matrix)\n\n\nNumerical Data Summary:\n       Temperature(F)     Start_Lat     Start_Lng          Hour\ncount    6.805138e+06  6.805138e+06  6.805138e+06  6.805138e+06\nmean     6.169845e+01  3.621612e+01 -9.476481e+01  1.227399e+01\nstd      1.891321e+01  5.073536e+00  1.732566e+01  5.446748e+00\nmin     -8.900000e+01  2.455480e+01 -1.246238e+02  0.000000e+00\n25%      4.900000e+01  3.340301e+01 -1.172136e+02  8.000000e+00\n50%      6.400000e+01  3.579237e+01 -8.790092e+01  1.300000e+01\n75%      7.600000e+01  4.011233e+01 -8.042281e+01  1.700000e+01\nmax      2.070000e+02  4.900220e+01 -6.711317e+01  2.300000e+01\n\nCategorical Data Summary:\n          State     City Weather_Condition\ncount   6805138  6805138           6805138\nunique       49    13150               142\ntop          CA  Houston              Fair\nfreq    1521976   157540           2196786\n\nMissing Values:\nID                   0\nStart_Time           0\nEnd_Time             0\nState                0\nCity                 0\nStart_Lat            0\nStart_Lng            0\nTemperature(F)       0\nWeather_Condition    0\nHour                 0\ndtype: int64\n\nCorrelation Matrix:\n                Temperature(F)  Start_Lat  Start_Lng      Hour\nTemperature(F)        1.000000  -0.440796  -0.015966  0.190139\nStart_Lat            -0.440796   1.000000  -0.065729 -0.010010\nStart_Lng            -0.015966  -0.065729   1.000000 -0.012075\nHour                  0.190139  -0.010010  -0.012075  1.000000"
  },
  {
    "objectID": "notebooks/data_cleaning.html",
    "href": "notebooks/data_cleaning.html",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "",
    "text": "Data Retrieval"
  },
  {
    "objectID": "notebooks/data_cleaning.html#data-source",
    "href": "notebooks/data_cleaning.html#data-source",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "1.1 Data Source",
    "text": "1.1 Data Source\nThe dataset used in this project is the US Accidents (3.0 Million records) dataset sourced from Kaggle.\n\nSource: Kaggle - US Accidents Dataset\nFormat: CSV\nContents: Accident data including location, time, weather conditions, and environment features.\n\n\n\nCode\n# Import required libraries\nimport pandas as pd\n\n# Load the accidents data\naccidents_df = pd.read_csv('../data/US_Accidents_March23.csv')  # Adjust path if needed\n\n# Show basic information\nprint(f\"Shape of dataset: {accidents_df.shape}\")\naccidents_df.head()\n\n\nShape of dataset: (7728394, 46)\n\n\n\n\n\n\n\n\n\nID\nSource\nSeverity\nStart_Time\nEnd_Time\nStart_Lat\nStart_Lng\nEnd_Lat\nEnd_Lng\nDistance(mi)\n...\nRoundabout\nStation\nStop\nTraffic_Calming\nTraffic_Signal\nTurning_Loop\nSunrise_Sunset\nCivil_Twilight\nNautical_Twilight\nAstronomical_Twilight\n\n\n\n\n0\nA-1\nSource2\n3\n2016-02-08 05:46:00\n2016-02-08 11:00:00\n39.865147\n-84.058723\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nNight\nNight\nNight\n\n\n1\nA-2\nSource2\n2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\n39.928059\n-82.831184\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nNight\nNight\nDay\n\n\n2\nA-3\nSource2\n2\n2016-02-08 06:49:27\n2016-02-08 07:19:27\n39.063148\n-84.032608\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nNight\nNight\nDay\nDay\n\n\n3\nA-4\nSource2\n3\n2016-02-08 07:23:34\n2016-02-08 07:53:34\n39.747753\n-84.205582\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nNight\nDay\nDay\nDay\n\n\n4\nA-5\nSource2\n2\n2016-02-08 07:39:07\n2016-02-08 08:09:07\n39.627781\n-84.188354\nNaN\nNaN\n0.01\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nDay\nDay\nDay\nDay\n\n\n\n\n5 rows √ó 46 columns\n\n\n\n2.1 Convert Time Columns to Datetime Format\n\n\nCode\n# 2.1 Convert Time Columns to Datetime Format\n# Convert 'Start_Time' and 'End_Time' to proper datetime objects\naccidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], errors='coerce')\naccidents_df['End_Time'] = pd.to_datetime(accidents_df['End_Time'], errors='coerce')\n\n# Confirm that conversion was successful\nprint(\"Start_Time type after conversion:\", accidents_df['Start_Time'].dtype)\nprint(\"End_Time type after conversion:\", accidents_df['End_Time'].dtype)\n\n\nStart_Time type after conversion: datetime64[ns]\nEnd_Time type after conversion: datetime64[ns]\n\n\n2.2 Create New Feature: Extract Hour from Start_Time\n\n\nCode\n# Extract hour of accident from Start_Time\naccidents_df['Hour'] = accidents_df['Start_Time'].dt.hour\n\n# View distribution of accidents by hour\naccidents_df['Hour'].value_counts().sort_index()\n\n\nHour\n0.0      98452\n1.0      85743\n2.0      82394\n3.0      74229\n4.0     149077\n5.0     209579\n6.0     375179\n7.0     546789\n8.0     541643\n9.0     334067\n10.0    313625\n11.0    322215\n12.0    316904\n13.0    352361\n14.0    394697\n15.0    463389\n16.0    520177\n17.0    516626\n18.0    390621\n19.0    267045\n20.0    201883\n21.0    169500\n22.0    148605\n23.0    110428\nName: count, dtype: int64\n\n\n2.3 Handle Missing Values\n\n\nCode\n# Check missing values summary\nmissing_values = accidents_df.isnull().sum()\nmissing_percentage = (missing_values / len(accidents_df)) * 100\n\nmissing_summary = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage (%)': missing_percentage\n})\nmissing_summary = missing_summary[missing_summary['Missing Values'] &gt; 0].sort_values(by='Percentage (%)', ascending=False)\n\nprint(\"Summary of Missing Values:\")\nmissing_summary\n\n\nSummary of Missing Values:\n\n\n\n\n\n\n\n\n\nMissing Values\nPercentage (%)\n\n\n\n\nEnd_Lat\n3402762\n44.029355\n\n\nEnd_Lng\n3402762\n44.029355\n\n\nPrecipitation(in)\n2203586\n28.512858\n\n\nWind_Chill(F)\n1999019\n25.865904\n\n\nEnd_Time\n743166\n9.616047\n\n\nStart_Time\n743166\n9.616047\n\n\nHour\n743166\n9.616047\n\n\nWind_Speed(mph)\n571233\n7.391355\n\n\nVisibility(mi)\n177098\n2.291524\n\n\nWind_Direction\n175206\n2.267043\n\n\nHumidity(%)\n174144\n2.253301\n\n\nWeather_Condition\n173459\n2.244438\n\n\nTemperature(F)\n163853\n2.120143\n\n\nPressure(in)\n140679\n1.820288\n\n\nWeather_Timestamp\n120228\n1.555666\n\n\nSunrise_Sunset\n23246\n0.300787\n\n\nCivil_Twilight\n23246\n0.300787\n\n\nNautical_Twilight\n23246\n0.300787\n\n\nAstronomical_Twilight\n23246\n0.300787\n\n\nAirport_Code\n22635\n0.292881\n\n\nStreet\n10869\n0.140637\n\n\nTimezone\n7808\n0.101030\n\n\nZipcode\n1915\n0.024779\n\n\nCity\n253\n0.003274\n\n\nDescription\n5\n0.000065\n\n\n\n\n\n\n\n2.4 Data columns to list\n\n\nCode\nprint(accidents_df.columns.tolist())\n\n\n['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description', 'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Hour']\n\n\n2.5 Data Enrichment\n\n\nCode\n# =====================================\n# 2. Feature Engineering (Data Enrichment)\n# =====================================\nimport numpy as np\n\n# Convert Start_Time and End_Time to datetime\naccidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], format='mixed', errors='coerce')\naccidents_df['End_Time'] = pd.to_datetime(accidents_df['End_Time'], format='mixed', errors='coerce')\n\n# 2.1 Create Duration feature (in minutes)\naccidents_df['Duration_minutes'] = (accidents_df['End_Time'] - accidents_df['Start_Time']).dt.total_seconds() / 60\n\n# 2.2 Extract Start Hour, Day of Week, and Month from Start_Time\naccidents_df['Start_Hour'] = accidents_df['Start_Time'].dt.hour\naccidents_df['Start_DayOfWeek'] = accidents_df['Start_Time'].dt.dayofweek\naccidents_df['Start_Month'] = accidents_df['Start_Time'].dt.month\n\n# 2.3 Categorize accidents into Day or Night\n# Day: 6 AM to 6 PM, otherwise Night\naccidents_df['Day_Night'] = np.where((accidents_df['Start_Hour'] &gt;= 6) & (accidents_df['Start_Hour'] &lt;= 18), 'Day', 'Night')\n\n# 2.4 Create Weekend flag\n# 1 if Saturday or Sunday, else 0\naccidents_df['Is_Weekend'] = accidents_df['Start_DayOfWeek'].apply(lambda x: 1 if x &gt;= 5 else 0)\n\n# 2.5 Create Season from Month (Optional but cool enrichment)\ndef map_season(month):\n    if month in [12, 1, 2]:\n        return 'Winter'\n    elif month in [3, 4, 5]:\n        return 'Spring'\n    elif month in [6, 7, 8]:\n        return 'Summer'\n    else:\n        return 'Fall'\n\naccidents_df['Season'] = accidents_df['Start_Month'].apply(map_season)\n\n# Preview the enriched dataset\nprint(f\"Shape after feature engineering: {accidents_df.shape}\")\ndisplay(accidents_df.head())\n\n\nShape after feature engineering: (7728394, 54)\n\n\n\n\n\n\n\n\n\nID\nSource\nSeverity\nStart_Time\nEnd_Time\nStart_Lat\nStart_Lng\nEnd_Lat\nEnd_Lng\nDistance(mi)\n...\nNautical_Twilight\nAstronomical_Twilight\nHour\nDuration_minutes\nStart_Hour\nStart_DayOfWeek\nStart_Month\nDay_Night\nIs_Weekend\nSeason\n\n\n\n\n0\nA-1\nSource2\n3\n2016-02-08 05:46:00\n2016-02-08 11:00:00\n39.865147\n-84.058723\nNaN\nNaN\n0.01\n...\nNight\nNight\n5.0\n314.0\n5.0\n0.0\n2.0\nNight\n0\nWinter\n\n\n1\nA-2\nSource2\n2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\n39.928059\n-82.831184\nNaN\nNaN\n0.01\n...\nNight\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n2\nA-3\nSource2\n2\n2016-02-08 06:49:27\n2016-02-08 07:19:27\n39.063148\n-84.032608\nNaN\nNaN\n0.01\n...\nDay\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n3\nA-4\nSource2\n3\n2016-02-08 07:23:34\n2016-02-08 07:53:34\n39.747753\n-84.205582\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n4\nA-5\nSource2\n2\n2016-02-08 07:39:07\n2016-02-08 08:09:07\n39.627781\n-84.188354\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n\n\n5 rows √ó 54 columns\n\n\n\n2.6 Data Cleaning\n\n\nCode\n# =====================================\n# Basic Data Cleaning\n# =====================================\n\n# Assume accidents_df is already loaded\n\n# 1.1 Drop duplicate rows based on ID\naccidents_df = accidents_df.drop_duplicates(subset=['ID'])\n\n# 1.2 Drop rows with critical missing values\naccidents_df = accidents_df.dropna(subset=['City', 'Weather_Condition', 'Temperature(F)'])\n\n# 1.3 Ensure Start_Time is before End_Time\naccidents_df = accidents_df[accidents_df['End_Time'] &gt;= accidents_df['Start_Time']]\n\n# 1.4 Reset index\naccidents_df = accidents_df.reset_index(drop=True)\n\nprint(f\"Shape after basic cleaning: {accidents_df.shape}\")\ndisplay(accidents_df.head())\n\n\nShape after basic cleaning: (6805138, 54)\n\n\n\n\n\n\n\n\n\nID\nSource\nSeverity\nStart_Time\nEnd_Time\nStart_Lat\nStart_Lng\nEnd_Lat\nEnd_Lng\nDistance(mi)\n...\nNautical_Twilight\nAstronomical_Twilight\nHour\nDuration_minutes\nStart_Hour\nStart_DayOfWeek\nStart_Month\nDay_Night\nIs_Weekend\nSeason\n\n\n\n\n0\nA-1\nSource2\n3\n2016-02-08 05:46:00\n2016-02-08 11:00:00\n39.865147\n-84.058723\nNaN\nNaN\n0.01\n...\nNight\nNight\n5.0\n314.0\n5.0\n0.0\n2.0\nNight\n0\nWinter\n\n\n1\nA-2\nSource2\n2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\n39.928059\n-82.831184\nNaN\nNaN\n0.01\n...\nNight\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n2\nA-3\nSource2\n2\n2016-02-08 06:49:27\n2016-02-08 07:19:27\n39.063148\n-84.032608\nNaN\nNaN\n0.01\n...\nDay\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n3\nA-4\nSource2\n3\n2016-02-08 07:23:34\n2016-02-08 07:53:34\n39.747753\n-84.205582\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n4\nA-5\nSource2\n2\n2016-02-08 07:39:07\n2016-02-08 08:09:07\n39.627781\n-84.188354\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n\n\n5 rows √ó 54 columns\n\n\n\n2.7 Handling Missing Data\n\n\nCode\n# =====================================\n# Handling Missing Data in Secondary Fields\n# =====================================\n\n# Fill missing Visibility (mi) with dataset mean\nif 'Visibility(mi)' in accidents_df.columns:\n    accidents_df['Visibility(mi)'] = accidents_df['Visibility(mi)'].fillna(accidents_df['Visibility(mi)'].mean())\n\n# Fill missing Wind Speed (mph) with dataset median\nif 'Wind_Speed(mph)' in accidents_df.columns:\n    accidents_df['Wind_Speed(mph)'] = accidents_df['Wind_Speed(mph)'].fillna(accidents_df['Wind_Speed(mph)'].median())\n\n# Fill missing Precipitation (in) with 0 (assume no rain if missing)\nif 'Precipitation(in)' in accidents_df.columns:\n    accidents_df['Precipitation(in)'] = accidents_df['Precipitation(in)'].fillna(0)\n\nprint(f\"Shape after handling missing secondary fields: {accidents_df.shape}\")\ndisplay(accidents_df.head())\n\n\nShape after handling missing secondary fields: (6805138, 54)\n\n\n\n\n\n\n\n\n\nID\nSource\nSeverity\nStart_Time\nEnd_Time\nStart_Lat\nStart_Lng\nEnd_Lat\nEnd_Lng\nDistance(mi)\n...\nNautical_Twilight\nAstronomical_Twilight\nHour\nDuration_minutes\nStart_Hour\nStart_DayOfWeek\nStart_Month\nDay_Night\nIs_Weekend\nSeason\n\n\n\n\n0\nA-1\nSource2\n3\n2016-02-08 05:46:00\n2016-02-08 11:00:00\n39.865147\n-84.058723\nNaN\nNaN\n0.01\n...\nNight\nNight\n5.0\n314.0\n5.0\n0.0\n2.0\nNight\n0\nWinter\n\n\n1\nA-2\nSource2\n2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\n39.928059\n-82.831184\nNaN\nNaN\n0.01\n...\nNight\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n2\nA-3\nSource2\n2\n2016-02-08 06:49:27\n2016-02-08 07:19:27\n39.063148\n-84.032608\nNaN\nNaN\n0.01\n...\nDay\nDay\n6.0\n30.0\n6.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n3\nA-4\nSource2\n3\n2016-02-08 07:23:34\n2016-02-08 07:53:34\n39.747753\n-84.205582\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n4\nA-5\nSource2\n2\n2016-02-08 07:39:07\n2016-02-08 08:09:07\n39.627781\n-84.188354\nNaN\nNaN\n0.01\n...\nDay\nDay\n7.0\n30.0\n7.0\n0.0\n2.0\nDay\n0\nWinter\n\n\n\n\n5 rows √ó 54 columns\n\n\n\n2.8 Selecting columns for analysis\n\n\nCode\n# Select Useful Columns for Analysis\n\nuseful_columns = [\n    'ID', 'Start_Time', 'End_Time', 'State', 'City',\n    'Start_Lat', 'Start_Lng', 'Temperature(F)', 'Weather_Condition', 'Hour'\n]\n\naccidents_df = accidents_df[useful_columns]\n\n# Check the shape and preview of the cleaned dataset\nprint(f\"Shape after selecting useful columns: {accidents_df.shape}\")\naccidents_df.head()\n\n\nShape after selecting useful columns: (6805138, 10)\n\n\n\n\n\n\n\n\n\nID\nStart_Time\nEnd_Time\nState\nCity\nStart_Lat\nStart_Lng\nTemperature(F)\nWeather_Condition\nHour\n\n\n\n\n0\nA-1\n2016-02-08 05:46:00\n2016-02-08 11:00:00\nOH\nDayton\n39.865147\n-84.058723\n36.9\nLight Rain\n5.0\n\n\n1\nA-2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\nOH\nReynoldsburg\n39.928059\n-82.831184\n37.9\nLight Rain\n6.0\n\n\n2\nA-3\n2016-02-08 06:49:27\n2016-02-08 07:19:27\nOH\nWilliamsburg\n39.063148\n-84.032608\n36.0\nOvercast\n6.0\n\n\n3\nA-4\n2016-02-08 07:23:34\n2016-02-08 07:53:34\nOH\nDayton\n39.747753\n-84.205582\n35.1\nMostly Cloudy\n7.0\n\n\n4\nA-5\n2016-02-08 07:39:07\n2016-02-08 08:09:07\nOH\nDayton\n39.627781\n-84.188354\n36.0\nMostly Cloudy\n7.0\n\n\n\n\n\n\n\n2.9 Sanity Checks after Data Cleaning\n\n\nCode\n# Assert no missing values in selected useful columns\nassert accidents_df[['ID', 'Start_Time', 'End_Time', 'State', 'City', \n                     'Start_Lat', 'Start_Lng', 'Temperature(F)', 'Weather_Condition', 'Hour']].isnull().sum().sum() == 0, \"There are still missing values!\"\n\n# Assert Start_Time is before End_Time\nassert (accidents_df['Start_Time'] &lt;= accidents_df['End_Time']).all(), \"Found rows where Start_Time is after End_Time!\"\n\nprint(\" Sanity checks passed: No missing values, Start_Time before End_Time.\")\n\n\n Sanity checks passed: No missing values, Start_Time before End_Time.\n\n\n2.10 Backup of cleaned data\n\n\nCode\n# Saving the cleaned dataframe into CSV, Excel, and Parquet formats\n# Save CSV (full)\naccidents_df.to_csv('accidents_cleaned.csv', index=False)\n\n# Save Excel (only first 1 million rows to respect Excel limits)\naccidents_df.head(1000000).to_excel('accidents_cleaned.xlsx', index=False)\n\n# Save Parquet (pickle)\naccidents_df.to_pickle('us_accidents_cleaned.pkl')\n\nprint(\"Data saved: CSV (full), Excel, Parquet (full)\")\n\n\nData saved: CSV (full), Excel, Parquet (full)"
  },
  {
    "objectID": "website.html",
    "href": "website.html",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "",
    "text": "This project aims to explore traffic accident patterns in the United States using a comprehensive dataset of over 6.8 million records from 2016 to 2023. We sought to uncover trends, analyze environmental and temporal factors, and apply regression modeling to predict accident duration and frequency."
  },
  {
    "objectID": "website.html#project-objective",
    "href": "website.html#project-objective",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "",
    "text": "This project aims to explore traffic accident patterns in the United States using a comprehensive dataset of over 6.8 million records from 2016 to 2023. We sought to uncover trends, analyze environmental and temporal factors, and apply regression modeling to predict accident duration and frequency."
  },
  {
    "objectID": "website.html#data-source",
    "href": "website.html#data-source",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "2. Data Source",
    "text": "2. Data Source\n‚Ä¢‚Å† ‚Å†Dataset: US Accidents (3.0 Million records)\n‚Ä¢‚Å† ‚Å†Provider: Sobhan Moosavi (via Kaggle)\n‚Ä¢‚Å† ‚Å†Scope: Covers 49 U.S. states (excluding Alaska and Hawaii)\n‚Ä¢‚Å† ‚Å†Time Period: February 2016 ‚Äì March 2023\n‚Ä¢‚Å† ‚Å†Fields: Accident timestamps, location, weather, road attributes, and traffic signals."
  },
  {
    "objectID": "website.html#data-cleaning-enrichment",
    "href": "website.html#data-cleaning-enrichment",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "3. Data Cleaning & Enrichment",
    "text": "3. Data Cleaning & Enrichment\n‚Ä¢‚Å† ‚Å†Converted ‚Å†‚ÄØStart_Time‚ÄØ‚Å† and ‚Å†‚ÄØEnd_Time‚ÄØ‚Å† to datetime objects\n‚Ä¢‚Å† ‚Å†Created new features: ‚Å†‚ÄØDuration_minutes‚ÄØ‚Å†, ‚Å†‚ÄØHour‚ÄØ‚Å†, ‚Å†‚ÄØDayOfWeek‚ÄØ‚Å†, ‚Å†‚ÄØMonth‚ÄØ‚Å†, ‚Å†‚ÄØIs_Weekend‚ÄØ‚Å†, ‚Å†‚ÄØDay/Night‚ÄØ‚Å†, and ‚Å†‚ÄØSeason‚ÄØ‚Å†\n‚Ä¢‚Å† ‚Å†Handled missing values and selected relevant columns for analysis\n‚Ä¢‚Å† ‚Å†Final dataset was tidy, consistent, and analysis-ready"
  },
  {
    "objectID": "website.html#data-exploration-highlights",
    "href": "website.html#data-exploration-highlights",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "4. Data Exploration Highlights",
    "text": "4. Data Exploration Highlights\n‚Ä¢‚Å† ‚Å†Rush Hour Peaks: Accidents spike around 7‚Äì9 AM and 4‚Äì6 PM\n‚Ä¢‚Å† ‚Å†High-Risk States/Cities: California, Florida, and Texas report the most accidents\n‚Ä¢‚Å† ‚Å†Weather Insight: Most accidents happen in clear or fair weather, not during storms or snow\n‚Ä¢‚Å† ‚Å†Temperature Trend: Most accidents occur in moderate temperatures (32¬∞F‚Äì80¬∞F)\n‚Ä¢‚Å† ‚Å†Location Density: Accidents cluster around major highways and urban areas"
  },
  {
    "objectID": "website.html#data-visualization-summary",
    "href": "website.html#data-visualization-summary",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "5. Data Visualization Summary",
    "text": "5. Data Visualization Summary\n\ni.Distribution of Accidents by Hour\nThis plot shows the distribution of accidents across different hours of the day. It helps identify when accidents are more likely to occur.\nVariables:\n- X-axis: Hour of the day (from 0 to 23) ‚Äî This represents the time of day when accidents occurred.\n- Y-axis: Number of Accidents ‚Äî The total count of accidents that happened at each hour of the day.\nThe plot reveals patterns in accident occurrences based on time, helping to identify peak accident hours.\n\n\n\n\n\n\n\n\n\n\n\nii.Top 10 States with Most Accidents\nThis plot visualizes the distribution of accidents across the top 10 states. It helps identify which states have the highest accident rates.\nVariables:\n\nLabels: States (e.g., ‚ÄúCalifornia‚Äù, ‚ÄúNew York‚Äù)\nValues: Number of Accidents ‚Äî The number of accidents that occurred in each state.\n\n\n\n\n\n\n\n\n\n\n\n\niii.Top 10 Cities with Most Accidents\nThis bar plot shows the top 10 cities with the most accidents. It gives insights into the cities where accidents are most frequent.\nVariables:\n\nX-axis: Number of Accidents ‚Äî The total count of accidents in each city.\nY-axis: Cities ‚Äî The names of the cities.\n\n\n\n\n\n\n\n\n\n\n\n\niv.Top 10 Weather Conditions during Accidents\nThis bar plot displays the top 10 weather conditions under which accidents occurred.\nVariables:\n\nX-axis: Weather Conditions (e.g., ‚ÄúClear‚Äù, ‚ÄúRain‚Äù)\nY-axis: Number of Accidents ‚Äî The number of accidents that occurred under each weather condition.\n\n\n\n\n\n\n\n\n\n\n\n\nv.Temperature Distribution during Accidents\nThis box plot compares temperatures during accidents on weekdays and weekends.\nVariables:\n\nX-axis: Is_Weekend (1 for Weekend, 0 for Weekday)\nY-axis: Temperature (Fahrenheit) ‚Äî The temperature at the time of the accident.\n\n\n\n\n\n\n\n\n\n\n\n\nvi.Accident Locations in USA\nThis scatter plot visualizes accident locations across the United States using geographic coordinates.\nVariables:\n\nX-axis: Longitude ‚Äî The geographic coordinate representing the east-west position.\nY-axis: Latitude ‚Äî The geographic coordinate representing the north-south position.\n\n\n\n\n\n\n\n\n\n\n\n\nvii.Interactive Plot: Number of Accidents by Hour\nThis interactive plot shows the number of accidents across different hours of the day. It‚Äôs interactive, so you can hover over each bar to see the exact number of accidents.\nVariables:\n\nX-axis: Hour of the Day ‚Äî The time of day when accidents occurred.\nY-axis: Number of Accidents ‚Äî The total number of accidents that occurred at each hour."
  },
  {
    "objectID": "website.html#regression-modeling",
    "href": "website.html#regression-modeling",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "6. Regression Modeling",
    "text": "6. Regression Modeling\n\n6.1 Objective\nPredict accident duration and frequency using features like temperature, weather, state, and city.\n\n\n6.2 Models Used\n‚Ä¢‚Å† ‚Å†Linear Regression (baseline model)\n‚Ä¢‚Å† ‚Å†Ridge Regression (with L2 regularization)\n\n\n6.3 Results\n‚Ä¢‚Å† ‚Å†Both models had very low R¬≤ (0.02) and high MSE (~17.96B)\n‚Ä¢‚Å† ‚Å†Ridge offered no performance improvement\n‚Ä¢‚Å† ‚Å†Coefficients showed minor influence from selected features\n\n\n6.4 Interpretation\n‚Ä¢‚Å† ‚Å†Weak model performance suggests missing important predictors (e.g., road type, traffic density)\n‚Ä¢‚Å† ‚Å†Future work should explore non-linear models (e.g., Random Forest, XGBoost) for better accuracy"
  },
  {
    "objectID": "website.html#project-access",
    "href": "website.html#project-access",
    "title": "UNCOVERING PATTERNS IN US ACCIDENTS - MSDS 597 Project (Group 17)",
    "section": "7. Project Access",
    "text": "7. Project Access\n‚Ä¢‚Å† ‚Å†üìò Notebook version:\nnbviewer link\n‚Ä¢‚Å† ‚Å†üåê Website version:\nGitHub Pages website\n‚Ä¢‚Å† ‚Å†üß™ Processed data:\nGoogle Drive link"
  },
  {
    "objectID": "notebooks/data_exploration.html",
    "href": "notebooks/data_exploration.html",
    "title": "3. DATA EXPLORATION",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\n\naccidents_df = pd.read_csv(\"/Users/jeesh/Desktop/2ndSem/DataWrangling(Proj)/accidents_cleaned.csv\") #change as needed\naccidents_df.head()\n\n\n\n\n\n\n\n\n\nID\nStart_Time\nEnd_Time\nState\nCity\nStart_Lat\nStart_Lng\nTemperature(F)\nWeather_Condition\nHour\n\n\n\n\n0\nA-1\n2016-02-08 05:46:00\n2016-02-08 11:00:00\nOH\nDayton\n39.865147\n-84.058723\n36.9\nLight Rain\n5.0\n\n\n1\nA-2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\nOH\nReynoldsburg\n39.928059\n-82.831184\n37.9\nLight Rain\n6.0\n\n\n2\nA-3\n2016-02-08 06:49:27\n2016-02-08 07:19:27\nOH\nWilliamsburg\n39.063148\n-84.032608\n36.0\nOvercast\n6.0\n\n\n3\nA-4\n2016-02-08 07:23:34\n2016-02-08 07:53:34\nOH\nDayton\n39.747753\n-84.205582\n35.1\nMostly Cloudy\n7.0\n\n\n4\nA-5\n2016-02-08 07:39:07\n2016-02-08 08:09:07\nOH\nDayton\n39.627781\n-84.188354\n36.0\nMostly Cloudy\n7.0\n\n\n\n\n\n\n\n\n3.1 Distribution of Accidents by¬†Hour\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\", palette=\"Set2\")\n\n# Ensure Hour column exists\nif 'Hour' not in accidents_df.columns:\n    accidents_df['Hour'] = accidents_df['Start_Time'].dt.hour\n\nplt.figure(figsize=(10,6))\nsns.histplot(accidents_df['Hour'], bins=24, kde=False, color='coral')\nplt.title('Distribution of Accidents by Hour')\nplt.xlabel('Hour of Day')\nplt.ylabel('Number of Accidents')\nplt.xticks(range(0,24))\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2 Top 10 States with Most Accidents\n\n\nCode\ntop_states = accidents_df['State'].value_counts().head(10)\n\nplt.figure(figsize=(8,8))\nplt.pie(top_states.values, labels=top_states.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\nplt.title('Top 10 States with Most Accidents')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.3 Top 10 Cities with Most Accidents\n\n\nCode\ntop_cities = accidents_df['City'].value_counts().head(10)\n\nplt.figure(figsize=(10,6))\nsns.barplot(y=top_cities.index, x=top_cities.values, palette=\"Blues_d\")\nplt.title('Top 10 Cities with Most Accidents')\nplt.xlabel('Number of Accidents')\nplt.ylabel('City')\nplt.show()\n\n\nC:\\Users\\jeesh\\AppData\\Local\\Temp\\ipykernel_17332\\3331938128.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(y=top_cities.index, x=top_cities.values, palette=\"Blues_d\")\n\n\n\n\n\n\n\n\n\n\n\n3.4 Top 10 Weather Conditions during Accidents\n\n\nCode\ntop_weather = accidents_df['Weather_Condition'].value_counts().head(10)\nweather_df = top_weather.reset_index()\nweather_df.columns = ['Weather', 'Counts']\n\nplt.figure(figsize=(12,6))\nsns.barplot(x='Weather', y='Counts', data=weather_df, palette='cubehelix')\nplt.title('Top 10 Weather Conditions during Accidents')\nplt.xlabel('Weather Condition')\nplt.ylabel('Number of Accidents')\nplt.xticks(rotation=45)\nplt.show()\n\n\nC:\\Users\\jeesh\\AppData\\Local\\Temp\\ipykernel_17332\\1489753945.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Weather', y='Counts', data=weather_df, palette='cubehelix')\n\n\n\n\n\n\n\n\n\n\n\n3.5 Temperature Distribution during Accidents\n\n\nCode\naccidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], errors='coerce')\n\n# Ensure Is_Weekend column exists\nif 'Is_Weekend' not in accidents_df.columns:\n    accidents_df['Weekday'] = accidents_df['Start_Time'].dt.weekday\n    accidents_df['Is_Weekend'] = accidents_df['Weekday'].apply(lambda x: 1 if x &gt;= 5 else 0)\n\nplt.figure(figsize=(10,6))\nsns.boxplot(x='Is_Weekend', y='Temperature(F)', data=accidents_df, palette=\"coolwarm\")\nplt.title('Temperature during Accidents (Weekday vs Weekend)')\nplt.xlabel('Weekend (1) vs Weekday (0)')\nplt.ylabel('Temperature (F)')\nplt.show()\n\n\nC:\\Users\\jeesh\\AppData\\Local\\Temp\\ipykernel_17332\\3257185058.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='Is_Weekend', y='Temperature(F)', data=accidents_df, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n\n3.6 Accident Locations in USA\n\n\nCode\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=accidents_df, alpha=0.1, s=10, color='purple')\nplt.title('Accident Locations in USA')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.7 Interactive plot: Number of accidents by hour\n\n\nCode\nimport plotly.express as px\n \nhourly_counts = accidents_df['Hour'].value_counts().sort_index()\nfig = px.bar(x=hourly_counts.index, y=hourly_counts.values,\n             labels={'x': 'Hour of Day', 'y': 'Number of Accidents'},\n             title='Number of Accidents by Hour',\n             color_discrete_sequence=[\"indianred\"])\nfig.update_layout(xaxis=dict(tickmode='linear', dtick=1))\nfig.show()"
  },
  {
    "objectID": "notebooks/data_analysis_Part1.html",
    "href": "notebooks/data_analysis_Part1.html",
    "title": "4. DATA ANALYSIS (PART 1)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\n\naccidents_df = pd.read_csv(\"/Users/jeesh/Desktop/2ndSem/DataWrangling(Proj)/accidents_cleaned.csv\") #change as needed\naccidents_df.head()\n\n\n\n\n\n\n\n\n\nID\nStart_Time\nEnd_Time\nState\nCity\nStart_Lat\nStart_Lng\nTemperature(F)\nWeather_Condition\nHour\n\n\n\n\n0\nA-1\n2016-02-08 05:46:00\n2016-02-08 11:00:00\nOH\nDayton\n39.865147\n-84.058723\n36.9\nLight Rain\n5.0\n\n\n1\nA-2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\nOH\nReynoldsburg\n39.928059\n-82.831184\n37.9\nLight Rain\n6.0\n\n\n2\nA-3\n2016-02-08 06:49:27\n2016-02-08 07:19:27\nOH\nWilliamsburg\n39.063148\n-84.032608\n36.0\nOvercast\n6.0\n\n\n3\nA-4\n2016-02-08 07:23:34\n2016-02-08 07:53:34\nOH\nDayton\n39.747753\n-84.205582\n35.1\nMostly Cloudy\n7.0\n\n\n4\nA-5\n2016-02-08 07:39:07\n2016-02-08 08:09:07\nOH\nDayton\n39.627781\n-84.188354\n36.0\nMostly Cloudy\n7.0\n\n\n\n\n\n\n\n\n4.1 What are the most dangerous hours?\n\n\nCode\nhourly_accidents = accidents_df['Hour'].value_counts().sort_index()\nprint(\"Accidents by Hour:\")\nprint(hourly_accidents)\n\n\nAccidents by Hour:\nHour\n0.0      94937\n1.0      82763\n2.0      79470\n3.0      71700\n4.0     144459\n5.0     203150\n6.0     364578\n7.0     533444\n8.0     528563\n9.0     325588\n10.0    305577\n11.0    314292\n12.0    309023\n13.0    343476\n14.0    384968\n15.0    452434\n16.0    508695\n17.0    505183\n18.0    381410\n19.0    260345\n20.0    196171\n21.0    164367\n22.0    143876\n23.0    106669\nName: count, dtype: int64\n\n\n\n\n4.2 Which states are most accident-prone?\n\n\nCode\nstate_accidents = accidents_df['State'].value_counts()\nprint(\"\\nTop States by Accident Count:\")\nprint(state_accidents)\n\n\n\nTop States by Accident Count:\nState\nCA    1521976\nFL     751644\nTX     535188\nSC     344085\nNY     307443\nNC     306223\nPA     255766\nVA     244974\nMN     168730\nIL     160399\nOR     159550\nGA     153275\nTN     151121\nMI     150204\nAZ     147804\nLA     134426\nNJ     120276\nOH     109971\nWA     101597\nMD      98777\nAL      94404\nUT      81781\nCO      81423\nOK      80710\nMO      68283\nCT      62056\nIN      60476\nMA      58754\nKY      31496\nWI      31314\nNE      28091\nIA      23395\nMT      21115\nNV      19372\nAR      17978\nRI      16019\nKS      15621\nDC      14732\nMS      13518\nDE      12714\nWV      10913\nNH      10034\nNM       9731\nID       8994\nME       2642\nWY       2514\nND       2488\nVT        886\nSD        255\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/data_analysis_Part2.html",
    "href": "notebooks/data_analysis_Part2.html",
    "title": "4. DATA ANALYSIS (PART 2)",
    "section": "",
    "text": "Code\nimport pandas as pd\naccidents_df = pd.read_csv(\"/Users/anshureddy/Desktop/dwproject/accidents_cleaned.csv\")\naccidents_df.head()\n\n\n\n\n\n\n\n\n\nID\nStart_Time\nEnd_Time\nState\nCity\nStart_Lat\nStart_Lng\nTemperature(F)\nWeather_Condition\nHour\n\n\n\n\n0\nA-1\n2016-02-08 05:46:00\n2016-02-08 11:00:00\nOH\nDayton\n39.865147\n-84.058723\n36.9\nLight Rain\n5.0\n\n\n1\nA-2\n2016-02-08 06:07:59\n2016-02-08 06:37:59\nOH\nReynoldsburg\n39.928059\n-82.831184\n37.9\nLight Rain\n6.0\n\n\n2\nA-3\n2016-02-08 06:49:27\n2016-02-08 07:19:27\nOH\nWilliamsburg\n39.063148\n-84.032608\n36.0\nOvercast\n6.0\n\n\n3\nA-4\n2016-02-08 07:23:34\n2016-02-08 07:53:34\nOH\nDayton\n39.747753\n-84.205582\n35.1\nMostly Cloudy\n7.0\n\n\n4\nA-5\n2016-02-08 07:39:07\n2016-02-08 08:09:07\nOH\nDayton\n39.627781\n-84.188354\n36.0\nMostly Cloudy\n7.0\n\n\n\n\n\n\n\n\n4.3. Which weather conditions are associated with most accidents?\n\n\nCode\n# Count the number of accidents under each unique weather condition\nweather_accidents = accidents_df['Weather_Condition'].value_counts()\nprint(\"\\nTop Weather Conditions by Accident Count:\")\n\n# Count the number of accidents under each unique weather condition\nprint(weather_accidents.head(10))\n\n\n\nTop Weather Conditions by Accident Count:\nWeather_Condition\nFair                2196786\nMostly Cloudy        921306\nClear                805417\nCloudy               712775\nPartly Cloudy        633258\nOvercast             381714\nLight Rain           321239\nScattered Clouds     204104\nLight Snow           112076\nFog                   87929\nName: count, dtype: int64\n\n\n\n\n4.4. Temperature vs Accidents (basic analysis)\n\n\nCode\n# Print average temperature during accidents\nprint(\"\\nTemperature Analysis:\")\nprint(f\"Average Temperature at Accident Time: {accidents_df['Temperature(F)'].mean():.2f} ¬∞F\")\n\n# Categorize accidents based on temperature ranges\n# Cold: &lt; 32¬∞F, Moderate: 32‚Äì80¬∞F, Hot: &gt; 80¬∞F\ncold_accidents = accidents_df[accidents_df['Temperature(F)'] &lt; 32].shape[0]\nmoderate_accidents = accidents_df[(accidents_df['Temperature(F)'] &gt;= 32) & (accidents_df['Temperature(F)'] &lt;= 80)].shape[0]\nhot_accidents = accidents_df[accidents_df['Temperature(F)'] &gt; 80].shape[0]\n\n# Print the number of accidents in each temperature category\nprint(f\"Accidents in cold (&lt;32F): {cold_accidents}\")\nprint(f\"Accidents in moderate (32-80F): {moderate_accidents}\")\nprint(f\"Accidents in hot (&gt;80F): {hot_accidents}\")\n\n\n\nTemperature Analysis:\nAverage Temperature at Accident Time: 61.70 ¬∞F\nAccidents in cold (&lt;32F): 480001\nAccidents in moderate (32-80F): 5175765\nAccidents in hot (&gt;80F): 1149372\n\n\n\n\n4.5. Accident Duration Statistics\n\n\nCode\n#Convert Start_Time and End_Time columns to datetime format\n# This is necessary to perform time-based operations like calculating Duration\n\naccidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], errors='coerce')\naccidents_df['End_Time'] = pd.to_datetime(accidents_df['End_Time'], errors='coerce')\n\n# Calculate accident duration in minutes\n# Subtracting Start_Time from End_Time gives a timedelta object, then convert to minutes\naccidents_df['Duration'] = (accidents_df['End_Time'] - accidents_df['Start_Time']).dt.total_seconds() / 60  # in minutes\n\n# Display summary statistics for accident duration\nprint(\"\\nAccident Duration Statistics (in minutes):\")\nprint(accidents_df['Duration'].describe())\n\n\n\nAccident Duration Statistics (in minutes):\ncount    6.805138e+06\nmean     4.282940e+02\nstd      1.328626e+04\nmin      1.216667e+00\n25%      3.000000e+01\n50%      6.240000e+01\n75%      1.225000e+02\nmax      2.812939e+06\nName: Duration, dtype: float64"
  }
]